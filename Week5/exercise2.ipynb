{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64aa4a01-3c1e-4fd0-9ad4-ec4106e5ac2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5f3b5a8-758c-41b0-a9de-1a103212ee73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image = \n",
      " tensor([[0.4456, 0.4011, 0.8302, 0.7384, 0.4079, 0.1627],\n",
      "        [0.0504, 0.7576, 0.2124, 0.2411, 0.7417, 0.1898],\n",
      "        [0.8516, 0.4527, 0.6020, 0.8988, 0.3174, 0.6074],\n",
      "        [0.1424, 0.9761, 0.0659, 0.2700, 0.0252, 0.4630],\n",
      "        [0.1924, 0.5014, 0.5550, 0.2047, 0.3048, 0.6525],\n",
      "        [0.0074, 0.2670, 0.2803, 0.2601, 0.1140, 0.9288]])\n"
     ]
    }
   ],
   "source": [
    "image = torch.rand(6, 6)\n",
    "\n",
    "print('Image = \\n', image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "208a936b-e50e-465b-9b8b-7173a49bb5b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape:  torch.Size([1, 6, 6])\n",
      "Image: \n",
      " tensor([[[0.4456, 0.4011, 0.8302, 0.7384, 0.4079, 0.1627],\n",
      "         [0.0504, 0.7576, 0.2124, 0.2411, 0.7417, 0.1898],\n",
      "         [0.8516, 0.4527, 0.6020, 0.8988, 0.3174, 0.6074],\n",
      "         [0.1424, 0.9761, 0.0659, 0.2700, 0.0252, 0.4630],\n",
      "         [0.1924, 0.5014, 0.5550, 0.2047, 0.3048, 0.6525],\n",
      "         [0.0074, 0.2670, 0.2803, 0.2601, 0.1140, 0.9288]]])\n"
     ]
    }
   ],
   "source": [
    "image = image.unsqueeze(dim=0)\n",
    "\n",
    "print('Image shape: ', image.shape)\n",
    "print('Image: \\n', image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f290dce9-5efc-47c4-8afd-077cdd33dc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = nn.Conv2d(1, 3, kernel_size=(3, 3), bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7f9dbe3-0018-4140-a296-121b17a7450f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OutImage shape =  torch.Size([3, 4, 4])\n",
      "OutImage = \n",
      " tensor([[[-0.7921, -0.5342, -0.7305, -0.7333],\n",
      "         [-0.3655, -0.4462, -0.3612, -0.1561],\n",
      "         [-0.8727, -0.3013, -0.5865, -0.4509],\n",
      "         [-0.4180, -0.4715, -0.0989, -0.3119]],\n",
      "\n",
      "        [[ 0.6715,  0.2896,  0.4062,  0.5630],\n",
      "         [-0.0750,  0.4828,  0.3203,  0.0345],\n",
      "         [ 0.6724,  0.2045,  0.3750,  0.4468],\n",
      "         [ 0.0911,  0.4517, -0.0242,  0.3537]],\n",
      "\n",
      "        [[-0.0433,  0.2252,  0.2945, -0.0139],\n",
      "         [ 0.1634,  0.1718,  0.0544,  0.2524],\n",
      "         [ 0.0573,  0.3006,  0.1307,  0.3503],\n",
      "         [ 0.1454,  0.1775,  0.0719,  0.1229]]], grad_fn=<SqueezeBackward1>)\n"
     ]
    }
   ],
   "source": [
    "outimage = conv(image)\n",
    "\n",
    "print('OutImage shape = ', outimage.shape)\n",
    "print('OutImage = \\n', outimage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a434591-dad2-41a3-8d8e-73151c465ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyConv2d:\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
    "        self.kernels = torch.rand(out_channels, 1, in_channels, kernel_size[0], kernel_size[1])\n",
    "        self.out_channels = out_channels\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "    def convolve(self, x):\n",
    "        outputs = [F.conv2d(x, self.kernels[i], stride=self.stride, padding=self.padding) for i in range(self.out_channels)]\n",
    "        return torch.concat(outputs, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fff3751-e53f-49ec-89c1-0296ec5dc979",
   "metadata": {},
   "outputs": [],
   "source": [
    "myconv = MyConv2d(1, 3, (3, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "daf7936c-5a9c-4c1e-a9d7-bb5b14e458f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OutImage shape =  torch.Size([3, 4, 4])\n",
      "OutImage = \n",
      " tensor([[[2.4235, 2.1604, 2.0828, 1.7772],\n",
      "         [1.8947, 1.6787, 1.8898, 1.1575],\n",
      "         [2.1721, 1.9367, 1.1285, 1.4318],\n",
      "         [1.1018, 1.3610, 0.7908, 1.4739]],\n",
      "\n",
      "        [[3.0231, 3.2784, 2.8141, 2.4024],\n",
      "         [2.4015, 2.4269, 2.0481, 2.2124],\n",
      "         [2.5010, 2.6062, 1.8098, 2.1686],\n",
      "         [1.8848, 1.5442, 1.1593, 2.3265]],\n",
      "\n",
      "        [[1.9658, 2.0455, 2.5431, 2.0869],\n",
      "         [1.6423, 2.0246, 1.1063, 1.4473],\n",
      "         [2.0629, 1.6575, 1.7440, 1.7706],\n",
      "         [1.4812, 1.5890, 0.7715, 1.2275]]])\n"
     ]
    }
   ],
   "source": [
    "outimage = myconv.convolve(image)\n",
    "\n",
    "print('OutImage shape = ', outimage.shape)\n",
    "print('OutImage = \\n', outimage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1716fc-3d9d-4e03-b19a-363c96a691bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
